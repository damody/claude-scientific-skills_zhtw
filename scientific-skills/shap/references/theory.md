# SHAP 理論基礎

本文件解釋 SHAP（SHapley Additive exPlanations）的理論基礎，包括博弈論中的 Shapley 值、使 SHAP 獨特的原則，以及與其他解釋方法的連接。

## 博弈論起源

### Shapley 值

SHAP 基於 **Shapley 值**，這是 Lloyd Shapley 於 1951 年開發的合作博弈論中的解概念。

**核心概念**：
在合作博弈論中，玩家合作以達成總收益，問題是：這個收益應該如何在玩家之間公平分配？

**映射到機器學習**：
- **玩家** → 輸入特徵
- **遊戲** → 模型預測任務
- **收益** → 模型輸出（預測值）
- **聯盟** → 具有已知值的特徵子集
- **公平分配** → 將預測歸因於特徵

### Shapley 值公式

對於特徵 $i$，其 Shapley 值 $\phi_i$ 為：

$$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \cup \{i\}) - f(S)]$$

其中：
- $F$ 是所有特徵的集合
- $S$ 是不包括 $i$ 的特徵子集
- $f(S)$ 是僅給定 $S$ 中特徵時模型的期望輸出
- $|S|$ 是子集 $S$ 的大小

**解釋**：
Shapley 值對特徵 $i$ 在所有可能特徵聯盟（子集）上的邊際貢獻進行平均。貢獻根據每個聯盟發生的可能性進行加權。

### Shapley 值的關鍵屬性

**1. 效率性（可加性）**：
$$\sum_{i=1}^{n} \phi_i = f(x) - f(\emptyset)$$

所有 SHAP 值的總和等於模型對該實例的預測與期望值（基準值）之間的差異。

這就是為什麼 SHAP 瀑布圖總是加總到總預測變化。

**2. 對稱性**：
如果兩個特徵 $i$ 和 $j$ 對所有聯盟的貢獻相等，則 $\phi_i = \phi_j$。

具有相同效果的特徵獲得相同的歸因。

**3. 虛擬性**：
如果特徵 $i$ 對任何聯盟都不改變模型輸出，則 $\phi_i = 0$。

不相關的特徵獲得零歸因。

**4. 單調性**：
如果特徵的邊際貢獻在聯盟間增加，其 Shapley 值也會增加。

## 從博弈論到機器學習

### 挑戰

計算精確的 Shapley 值需要在所有可能的特徵聯盟上評估模型：
- 對於 $n$ 個特徵，有 $2^n$ 個可能的聯盟
- 對於 50 個特徵，這超過 1 千萬億次評估

這種指數複雜度使得大多數實際模型的精確計算變得不可行。

### SHAP 的解決方案：加性特徵歸因

SHAP 將 Shapley 值連接到**加性特徵歸因方法**，實現高效計算。

**加性特徵歸因模型**：
$$g(z') = \phi_0 + \sum_{i=1}^{M} \phi_i z'_i$$

其中：
- $g$ 是解釋模型
- $z' \in \{0,1\}^M$ 表示特徵存在/缺失
- $\phi_i$ 是對特徵 $i$ 的歸因
- $\phi_0$ 是基準值（期望值）

SHAP 證明 **Shapley 值是滿足三個理想屬性的唯一歸因值**：局部準確性、缺失性和一致性。

## SHAP 屬性和保證

### 局部準確性

**屬性**：解釋與模型的輸出匹配：
$$f(x) = g(x') = \phi_0 + \sum_{i=1}^{M} \phi_i x'_i$$

**解釋**：SHAP 值精確地解釋模型的預測。這使瀑布圖能夠精確分解預測。

### 缺失性

**屬性**：如果特徵缺失（未觀察到），其歸因為零：
$$x'_i = 0 \Rightarrow \phi_i = 0$$

**解釋**：只有存在的特徵對解釋有貢獻。

### 一致性

**屬性**：如果模型改變使特徵的邊際貢獻對所有輸入增加（或保持不變），該特徵的歸因不應減少。

**解釋**：如果特徵對模型變得更重要，其 SHAP 值會反映這一點。這使得有意義的模型比較成為可能。

## SHAP 作為統一框架

SHAP 透過展示幾種現有解釋方法是特定假設下 Shapley 值的特例來統一它們。

### LIME（Local Interpretable Model-agnostic Explanations）

**LIME 的方法**：使用擾動樣本在預測周圍擬合局部線性模型。

**與 SHAP 的連接**：LIME 近似 Shapley 值，但樣本加權次優。SHAP 使用從 Shapley 值公式衍生的理論最優權重。

**關鍵差異**：LIME 的損失函數和取樣不保證一致性或精確可加性；SHAP 保證。

### DeepLIFT

**DeepLIFT 的方法**：透過與參考激活比較來反向傳播貢獻通過神經網路。

**與 SHAP 的連接**：DeepExplainer 使用 DeepLIFT，但在多個參考樣本上平均以近似條件期望，產生 Shapley 值。

### 層級相關性傳播（LRP）

**LRP 的方法**：透過反向傳播相關性分數通過各層來分解神經網路預測。

**與 SHAP 的連接**：LRP 是具有特定傳播規則的 SHAP 特例。SHAP 用 Shapley 值理論概括這些規則。

### 積分梯度

**積分梯度的方法**：沿從基準到輸入的路徑積分梯度。

**與 SHAP 的連接**：當使用單一參考點時，積分梯度為平滑模型近似 SHAP 值。

## SHAP 計算方法

不同的 SHAP 解釋器使用專門的演算法來為特定模型類型高效計算 Shapley 值。

### Tree SHAP（TreeExplainer）

**創新**：利用樹結構以多項式時間而非指數時間計算精確 Shapley 值。

**演算法**：
- 遍歷從根到葉的每條樹路徑
- 使用樹分割和權重計算特徵貢獻
- 在集成中的所有樹上聚合

**複雜度**：$O(TLD^2)$，其中 $T$ = 樹數量，$L$ = 最大葉節點數，$D$ = 最大深度

**關鍵優勢**：為基於樹的模型（XGBoost、LightGBM、隨機森林等）高效計算精確 Shapley 值

### Kernel SHAP（KernelExplainer）

**創新**：使用加權線性迴歸為任何模型估計 Shapley 值。

**演算法**：
- 根據 Shapley 核權重對聯盟（特徵子集）進行取樣
- 在每個聯盟上評估模型（缺失特徵由背景值替換）
- 擬合加權線性模型以估計特徵歸因

**複雜度**：$O(n \cdot 2^M)$ 但用較少的樣本近似

**關鍵優勢**：模型無關；適用於任何預測函數

**權衡**：比專門的解釋器慢；是近似而非精確

### Deep SHAP（DeepExplainer）

**創新**：結合 DeepLIFT 與 Shapley 值取樣。

**演算法**：
- 為每個參考樣本計算 DeepLIFT 歸因
- 在多個參考樣本上平均歸因
- 近似條件期望：$E[f(x) | x_S]$

**複雜度**：$O(n \cdot m)$，其中 $m$ = 參考樣本數量

**關鍵優勢**：為深度神經網路高效近似 Shapley 值

### Linear SHAP（LinearExplainer）

**創新**：線性模型的封閉形式 Shapley 值。

**演算法**：
- 對於獨立特徵：$\phi_i = w_i \cdot (x_i - E[x_i])$
- 對於相關特徵：調整特徵協方差

**複雜度**：$O(n)$ - 幾乎瞬間完成

**關鍵優勢**：以最小計算獲得精確 Shapley 值

## 理解條件期望

### 核心挑戰

計算 $f(S)$（僅給定 $S$ 中特徵時的模型輸出）需要處理缺失特徵。

**問題**：當模型需要所有特徵作為輸入時，我們應該如何表示「缺失」特徵？

### 兩種方法

**1. 介入性（邊際）方法**：
- 用來自背景資料集的值替換缺失特徵
- 估計：$E[f(x) | x_S]$，透過對 $x_{\bar{S}}$ 邊際化
- 解釋：「如果我們不知道特徵 $\bar{S}$，模型會預測什麼？」

**2. 觀測性（條件）方法**：
- 使用條件分佈：$E[f(x) | x_S = x_S^*]$
- 考慮特徵依賴關係
- 解釋：「對於具有特徵 $S = x_S^*$ 的相似實例，模型會預測什麼？」

**權衡**：
- **介入性**：更簡單，假設特徵獨立，匹配因果解釋
- **觀測性**：對相關特徵更準確，需要條件分佈估計

**TreeExplainer** 透過 `feature_perturbation` 參數支援兩種方法。

## 基準值（期望值）選擇

**基準值** $\phi_0 = E[f(x)]$ 代表模型的平均預測。

### 計算基準值

**對於 TreeExplainer**：
- 有背景資料：背景資料集上的平均預測
- 使用 tree_path_dependent：使用樹葉分佈的加權平均

**對於 DeepExplainer / KernelExplainer**：
- 背景樣本上的平均預測

### 基準值的重要性

- SHAP 值衡量與基準值的偏差
- 不同的基準值 → 不同的 SHAP 值（但仍正確加總）
- 選擇代表「典型」或「中性」輸入的基準值
- 常見選擇：訓練集平均值、中位數或眾數

## 解釋 SHAP 值

### 單位和尺度

**SHAP 值與模型輸出具有相同的單位**：
- 迴歸：與目標變數相同的單位（美元、溫度等）
- 分類（對數賠率）：對數賠率單位
- 分類（機率）：機率單位（如果模型輸出已轉換）

**大小**：較高的絕對 SHAP 值 = 較強的特徵影響

**符號**：
- 正 SHAP 值 = 特徵將預測推高
- 負 SHAP 值 = 特徵將預測推低

### 加性分解

對於預測 $f(x)$：
$$f(x) = E[f(X)] + \sum_{i=1}^{n} \phi_i(x)$$

**範例**：
- 期望值（基準值）：0.3
- SHAP 值：{Age: +0.15, Income: +0.10, Education: -0.05}
- 預測：$0.3 + 0.15 + 0.10 - 0.05 = 0.50$

### 全局 vs 局部重要性

**局部（實例層面）**：
- 單一預測的 SHAP 值：$\phi_i(x)$
- 解釋：「為什麼模型對這個實例預測 $f(x)$？」
- 視覺化：瀑布圖、力場圖

**全局（資料集層面）**：
- 平均絕對 SHAP 值：$E[|\phi_i(x)|]$
- 解釋：「整體上哪些特徵最重要？」
- 視覺化：蜂群圖、長條圖

**關鍵見解**：全局重要性是局部重要性的聚合，維持實例和資料集解釋之間的一致性。

## SHAP vs 其他特徵重要性方法

### 與排列重要性的比較

**排列重要性**：
- 打亂特徵並測量準確性下降
- 僅限全局指標（無實例層面解釋）
- 對相關特徵可能產生誤導

**SHAP**：
- 提供局部和全局重要性
- 透過聯盟平均處理特徵相關性
- 一致性：可加性屬性保證加總到預測

### 與特徵係數（線性模型）的比較

**特徵係數** ($w_i$)：
- 衡量每單位特徵變化的影響
- 不考慮特徵尺度或分佈

**線性模型的 SHAP**：
- $\phi_i = w_i \cdot (x_i - E[x_i])$
- 考慮相對於平均值的特徵值
- 對比較具有不同單位/尺度的特徵更具可解釋性

### 與樹特徵重要性（Gini/基於分割）的比較

**Gini/分割重要性**：
- 基於訓練過程（純度增益或分割頻率）
- 偏向高基數特徵
- 無實例層面解釋
- 可能產生誤導（重要性 ≠ 預測能力）

**SHAP（Tree SHAP）**：
- 基於模型輸出（預測行為）
- 透過 Shapley 值公平歸因
- 提供實例層面解釋
- 一致且具有理論基礎

## 互動和高階效應

### SHAP 互動值

標準 SHAP 捕捉主效應。**SHAP 互動值**捕捉成對互動。

**互動公式**：
$$\phi_{i,j} = \sum_{S \subseteq F \setminus \{i,j\}} \frac{|S|!(|F|-|S|-2)!}{2(|F|-1)!} \Delta_{ij}(S)$$

其中 $\Delta_{ij}(S)$ 是給定聯盟 $S$ 時特徵 $i$ 和 $j$ 的互動效應。

**解釋**：
- $\phi_{i,i}$：特徵 $i$ 的主效應
- $\phi_{i,j}$（$i \neq j$）：特徵 $i$ 和 $j$ 之間的互動效應

**屬性**：
$$\phi_i = \phi_{i,i} + \sum_{j \neq i} \phi_{i,j}$$

主 SHAP 值等於主效應加上涉及特徵 $i$ 的所有成對互動的一半。

### 計算互動

**TreeExplainer** 支援精確互動計算：
```python
explainer = shap.TreeExplainer(model)
shap_interaction_values = explainer.shap_interaction_values(X)
```

**限制**：對其他解釋器指數複雜（僅對樹模型實用）

## 理論限制和考量

### 計算複雜度

**精確計算**：$O(2^n)$ - 對大 $n$ 不可行

**專門演算法**：
- Tree SHAP：$O(TLD^2)$ - 對樹高效
- Deep SHAP、Kernel SHAP：需要近似

**含義**：對於具有許多特徵的非樹模型，解釋可能是近似的。

### 特徵獨立假設

**Kernel SHAP 和基本實現**：假設特徵可以獨立操作

**挑戰**：真實特徵通常是相關的（例如，身高和體重）

**解決方案**：
- 使用觀測性方法（條件期望）
- 使用相關性感知擾動的 TreeExplainer
- 對高度相關的特徵進行特徵分組

### 分佈外樣本

**問題**：透過替換特徵建立聯盟可能會建立不真實的樣本（在訓練分佈之外）

**範例**：同時設定「Age=5」和「Has PhD=Yes」

**含義**：SHAP 值反映模型在可能不真實輸入上的行為

**緩解**：使用觀測性方法或仔細選擇的背景資料

### 因果性

**SHAP 衡量關聯，而非因果**

SHAP 回答：「模型的預測如何隨這個特徵變化？」
SHAP 不回答：「如果我們在現實中改變這個特徵會發生什麼？」

**範例**：
- SHAP：「住院時間增加死亡率預測」（關聯）
- 因果：「更長的住院時間導致更高的死亡率」（不正確！）

**含義**：使用領域知識進行因果解釋 SHAP；SHAP 本身不建立因果關係。

## 進階理論主題

### SHAP 作為最優信用分配

SHAP 是滿足以下條件的唯一歸因方法：
1. **局部準確性**：解釋與模型匹配
2. **缺失性**：缺失特徵有零歸因
3. **一致性**：歸因反映特徵重要性變化

**證明**：Lundberg & Lee（2017）展示 Shapley 值是滿足這些公理的唯一解。

### 與函數 ANOVA 的連接

SHAP 值對應於函數 ANOVA 分解中的一階項：
$$f(x) = f_0 + \sum_i f_i(x_i) + \sum_{i,j} f_{ij}(x_i, x_j) + ...$$

其中 $f_i(x_i)$ 捕捉特徵 $i$ 的主效應，且 $\phi_i \approx f_i(x_i)$。

### 與敏感性分析的關係

SHAP 概括敏感性分析：
- **敏感性分析**：$\frac{\partial f}{\partial x_i}$（局部梯度）
- **SHAP**：在特徵聯盟空間上的積分敏感性

基於梯度的方法（GradientExplainer、積分梯度）使用導數近似 SHAP。

## 理論的實際含義

### 為什麼使用 SHAP？

1. **理論保證**：具有一致性、局部準確性和缺失性的唯一方法
2. **統一框架**：連接和概括多種解釋方法
3. **加性分解**：預測精確分解為特徵貢獻
4. **模型比較**：一致性使得跨模型比較特徵重要性成為可能
5. **多功能性**：適用於任何模型類型（使用適當的解釋器）

### 何時需要謹慎

1. **計算成本**：對於沒有專門解釋器的複雜模型可能較慢
2. **特徵相關性**：標準方法可能建立不真實的樣本
3. **解釋**：需要理解基準值、單位和假設
4. **因果性**：SHAP 不暗示因果關係；使用領域知識
5. **近似**：非樹方法使用近似；理解準確性權衡

## 參考文獻和進一步閱讀

**基礎論文**：
- Shapley, L. S.（1951）. 「A value for n-person games」
- Lundberg, S. M., & Lee, S. I.（2017）. 「A Unified Approach to Interpreting Model Predictions」（NeurIPS）
- Lundberg, S. M., et al.（2020）. 「From local explanations to global understanding with explainable AI for trees」（Nature Machine Intelligence）

**關鍵概念**：
- 合作博弈論和 Shapley 值
- 加性特徵歸因方法
- 條件期望估計
- Tree SHAP 演算法和多項式時間計算

這個理論基礎解釋了為什麼 SHAP 是一個有原則、多功能且強大的模型解釋工具。
