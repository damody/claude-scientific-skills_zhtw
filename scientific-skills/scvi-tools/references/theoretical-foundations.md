# scvi-tools 的理論基礎

本文檔解釋 scvi-tools 底層的數學和統計原理。

## 核心概念

### 變分推斷（Variational Inference）

**什麼是變分推斷？**
變分推斷是一種近似複雜機率分佈的技術。在單細胞分析中，我們想要理解後驗分佈 p(z|x) - 給定觀測數據 x 時潛在變數 z 的機率。

**為什麼使用它？**
- 對於複雜模型，精確推斷在計算上是不可行的
- 可擴展到大型數據集（數百萬細胞）
- 提供不確定性量化
- 支援關於細胞狀態的貝葉斯推理

**它是如何工作的？**
1. 定義一個更簡單的近似分佈 q(z|x)，具有可學習的參數
2. 最小化 q(z|x) 和真實後驗 p(z|x) 之間的 KL 散度
3. 等價於最大化證據下界（ELBO）

**ELBO 目標**：
```
ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))
       ↑                    ↑
    重建項                正則化項
```

- **重建項**：模型應生成與觀測相似的數據
- **正則化項**：潛在表示應匹配先驗

### 變分自編碼器（VAE）

**架構**：
```
x（觀測數據）
    ↓
[編碼器神經網路]
    ↓
z（潛在表示）
    ↓
[解碼器神經網路]
    ↓
x̂（重建數據）
```

**編碼器**：將細胞（x）映射到潛在空間（z）
- 學習 q(z|x)，近似後驗
- 由具有可學習權重的神經網路參數化
- 輸出潛在分佈的均值和方差

**解碼器**：將潛在空間（z）映射回基因空間
- 學習 p(x|z)，似然函數
- 從潛在表示生成基因表達
- 建模計數分佈（負二項分佈、零膨脹負二項分佈）

**重參數化技巧**：
- 允許通過隨機取樣進行反向傳播
- 取樣 z = μ + σ ⊙ ε，其中 ε ~ N(0,1)
- 支援使用梯度下降進行端到端訓練

### 攤銷推斷（Amortized Inference）

**概念**：跨所有細胞共享編碼器參數。

**傳統推斷**：為每個細胞學習單獨的潛在變數
- n_cells × n_latent 個參數
- 無法擴展到大型數據集

**攤銷推斷**：為所有細胞學習單一編碼器
- 無論細胞數量多少，參數數量固定
- 支援對新細胞的快速推斷
- 跨數據集遷移學習到的模式

**優點**：
- 可擴展到數百萬細胞
- 對查詢數據的快速推斷
- 利用細胞間的共享結構
- 支援少樣本學習

## 統計建模

### 計數數據分佈

單細胞數據是計數（整數值），需要適當的分佈。

#### 負二項分佈（NB）
```
x ~ NB(μ, θ)
```
- **μ（均值）**：期望表達水平
- **θ（離散度）**：控制方差
- **方差**：Var(x) = μ + μ²/θ

**何時使用**：沒有零膨脹的基因表達
- 比泊松更靈活（允許過度離散）
- 建模技術和生物學變異

#### 零膨脹負二項分佈（ZINB）
```
x ~ π·δ₀ + (1-π)·NB(μ, θ)
```
- **π（丟失率）**：技術零的機率
- **δ₀**：零處的點質量
- **NB(μ, θ)**：未丟失時的表達

**何時使用**：稀疏的 scRNA-seq 數據
- 將技術丟失與生物學零值分開建模
- 更適合高度稀疏的數據（例如 10x 數據）

#### 泊松分佈
```
x ~ Poisson(μ)
```
- 最簡單的計數分佈
- 均值等於方差：Var(x) = μ

**何時使用**：較少見；ATAC-seq 片段計數
- 比 NB 更有限制
- 計算更快

### 批次校正框架

**問題**：技術變異混淆生物學信號
- 不同的定序運行、協議、實驗室
- 必須移除技術效應同時保留生物學

**scvi-tools 方法**：
1. 將批次編碼為分類變數 s
2. 在生成模型中包含 s
3. 潛在空間 z 是批次不變的
4. 解碼器以 s 為條件處理批次特異性效應

**數學公式**：
```
編碼器：q(z|x, s)  - 批次感知編碼
潛在：z           - 批次校正的表示
解碼器：p(x|z, s)  - 批次特異性解碼
```

**關鍵洞察**：批次資訊流經解碼器，而非潛在空間
- z 捕捉生物學變異
- s 解釋技術變異
- 可分離的生物學和批次效應

### 深度生成建模

**生成模型**：學習 p(x)，數據分佈

**過程**：
1. 取樣潛在變數：z ~ p(z) = N(0, I)
2. 生成表達：x ~ p(x|z)
3. 聯合分佈：p(x, z) = p(x|z)p(z)

**優點**：
- 生成合成細胞
- 插補缺失值
- 量化不確定性
- 執行反事實預測

**推斷網路**：反轉生成過程
- 給定 x，推斷 z
- q(z|x) 近似真實後驗 p(z|x)

## 模型架構細節

### scVI 架構

**輸入**：基因表達計數 x ∈ ℕ^G（G 個基因）

**編碼器**：
```
h = ReLU(W₁·x + b₁)
μ_z = W₂·h + b₂
log σ²_z = W₃·h + b₃
z ~ N(μ_z, σ²_z)
```

**潛在空間**：z ∈ ℝ^d（通常 d=10-30）

**解碼器**：
```
h = ReLU(W₄·z + b₄)
μ = softmax(W₅·h + b₅) · library_size
θ = exp(W₆·h + b₆)
π = sigmoid(W₇·h + b₇)  # 用於 ZINB
x ~ ZINB(μ, θ, π)
```

**損失函數（ELBO）**：
```
L = E_q[log p(x|z)] - KL(q(z|x) || N(0,I))
```

### 處理共變量

**分類共變量**（批次、供體等）：
- 獨熱編碼：s ∈ {0,1}^K
- 與潛在連接：[z, s]
- 或使用條件層

**連續共變量**（文庫大小、percent_mito）：
- 標準化為零均值、單位方差
- 包含在編碼器和/或解碼器中

**共變量注入策略**：
- **連接**：[z, s] 輸入解碼器
- **深度注入**：s 在多層添加
- **條件批次標準化**：批次特異性標準化

## 進階理論概念

### 遷移學習（scArches）

**概念**：使用預訓練模型作為新數據的初始化

**過程**：
1. 在大型數據集上訓練參考模型
2. 凍結編碼器參數
3. 在查詢數據上微調解碼器
4. 或使用較低學習率微調全部

**為什麼有效**：
- 編碼器學習通用的細胞表示
- 解碼器適應查詢特異性特徵
- 防止災難性遺忘

**應用**：
- 查詢到參考映射
- 稀有細胞類型的少樣本學習
- 快速分析新數據集

### 多解析度建模（MrVI）

**思路**：分離共享和樣本特異性變異

**潛在空間分解**：
```
z = z_shared + z_sample
```
- **z_shared**：跨樣本通用
- **z_sample**：樣本特異性效應

**層次結構**：
```
樣本層級：ρ_s ~ N(0, I)
細胞層級：z_i ~ N(ρ_{s(i)}, σ²)
```

**優點**：
- 解開生物學變異來源
- 在不同解析度比較樣本
- 識別樣本特異性細胞狀態

### 反事實預測

**目標**：預測不同條件下的結果

**範例**：「如果這個細胞來自不同批次會怎樣？」

**方法**：
1. 將細胞編碼到潛在：z = Encoder(x, s_original)
2. 用新條件解碼：x_new = Decoder(z, s_new)
3. x_new 是反事實預測

**應用**：
- 批次效應評估
- 預測治療反應
- 電腦模擬擾動研究

### 後驗預測分佈

**定義**：給定觀測數據的新數據分佈

```
p(x_new | x_observed) = ∫ p(x_new|z) q(z|x_observed) dz
```

**估計**：從 q(z|x) 取樣 z，從 p(x_new|z) 生成 x_new

**用途**：
- 不確定性量化
- 穩健預測
- 異常值檢測

## 差異表達框架

### 貝葉斯方法

**傳統方法**：比較點估計
- Wilcoxon、t 檢驗等
- 忽略不確定性
- 需要偽計數

**scvi-tools 方法**：比較分佈
- 從後驗取樣：μ_A ~ p(μ|x_A)，μ_B ~ p(μ|x_B)
- 計算對數倍數變化：LFC = log(μ_B) - log(μ_A)
- LFC 的後驗分佈量化不確定性

### 貝葉斯因子

**定義**：後驗勝算與先驗勝算的比率

```
BF = P(H₁|data) / P(H₀|data)
     ─────────────────────────
     P(H₁) / P(H₀)
```

**解釋**：
- BF > 3：對 H₁ 的中等證據
- BF > 10：強證據
- BF > 100：決定性證據

**在 scvi-tools 中**：用於按 DE 證據對基因排序

### 錯誤發現比例（FDP）

**目標**：控制期望錯誤發現率

**程序**：
1. 對每個基因，計算 DE 的後驗機率
2. 按證據（貝葉斯因子）對基因排序
3. 選擇前 k 個基因使得 E[FDP] ≤ α

**相對於 p 值的優勢**：
- 完全貝葉斯
- 自然用於後驗推斷
- 沒有任意閾值

## 實作細節

### 優化

**優化器**：Adam（自適應學習率）
- 預設 lr = 0.001
- 動量參數：β₁=0.9，β₂=0.999

**訓練迴圈**：
1. 取樣細胞的小批量
2. 計算 ELBO 損失
3. 反向傳播梯度
4. 使用 Adam 更新參數
5. 重複直到收斂

**收斂標準**：
- ELBO 在驗證集上穩定
- 早停防止過擬合
- 通常 200-500 個 epoch

### 正則化

**KL 退火**：逐漸增加 KL 權重
- 防止後驗塌縮
- 從 0 開始，經過 epoch 增加到 1

**Dropout**：訓練期間隨機丟棄神經元
- 預設：0.1 丟棄率
- 防止過擬合
- 改善泛化

**權重衰減**：權重的 L2 正則化
- 防止大權重
- 改善穩定性

### 可擴展性

**小批量訓練**：
- 每次迭代處理細胞子集
- 批次大小：64-256 個細胞
- 支援擴展到數百萬細胞

**隨機優化**：
- 在小批量上估計 ELBO
- 無偏梯度估計
- 收斂到最優解

**GPU 加速**：
- 神經網路自然並行化
- 數量級加速
- 對大數據集至關重要

## 與其他方法的聯繫

### vs. PCA
- **PCA**：線性、確定性
- **scVI**：非線性、機率性
- **優勢**：scVI 捕捉複雜結構，處理計數

### vs. t-SNE/UMAP
- **t-SNE/UMAP**：專注於視覺化
- **scVI**：完整的生成模型
- **優勢**：scVI 支援下游任務（DE、插補）

### vs. Seurat 整合
- **Seurat**：基於錨點的對齊
- **scVI**：機率建模
- **優勢**：scVI 提供不確定性，適用於多批次

### vs. Harmony
- **Harmony**：PCA + 批次校正
- **scVI**：基於 VAE
- **優勢**：scVI 原生處理計數，更靈活

## 數學符號

**常用符號**：
- x：觀測基因表達（計數）
- z：潛在表示
- θ：模型參數
- q(z|x)：近似後驗（編碼器）
- p(x|z)：似然（解碼器）
- p(z)：潛在變數的先驗
- μ, σ²：均值和方差
- π：丟失機率（ZINB）
- θ（在 NB 中）：離散度參數
- s：批次/共變量指示器

## 延伸閱讀

**重要論文**：
1. Lopez et al. (2018)：「Deep generative modeling for single-cell transcriptomics」
2. Xu et al. (2021)：「Probabilistic harmonization and annotation of single-cell transcriptomics」
3. Boyeau et al. (2019)：「Deep generative models for detecting differential expression in single cells」

**要探索的概念**：
- 機器學習中的變分推斷
- 貝葉斯深度學習
- 資訊理論（KL 散度、互資訊）
- 生成模型（GAN、正規化流、擴散模型）
- 機率編程（Pyro、PyTorch）

**數學背景**：
- 機率論和統計學
- 線性代數和微積分
- 優化理論
- 資訊理論
